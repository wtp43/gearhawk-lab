apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: postgres-cluster
  namespace: cnpg-database
  annotations:
    argocd.argoproj.io/sync-wave: "2"
spec:
  imageName: ghcr.io/cloudnative-pg/postgresql:17.5
  imagePullPolicy: IfNotPresent
  instances: 1
  startDelay: 300
  stopDelay: 300
  primaryUpdateStrategy: unsupervised

  postgresql:
    # TODO: performance tuning https://www.enterprisedb.com/postgres-tutorials/introduction-postgresql-performance-tuning-and-optimization?lang=en
    # using: https://pgtune.leopard.in.ua/
    # WARNING: WAL tuning parameters should be the same as the backups
    parameters:
      pg_stat_statements.max: "10000"
      pg_stat_statements.track: all
      auto_explain.log_min_duration: "10s"
      shared_buffers: "16GB"
      effective_cache_size: "48GB"
      maintenance_work_mem: "2GB"
      checkpoint_completion_target: "0.9"
      wal_buffers: "16MB"
      default_statistics_target: "500"
      random_page_cost: "1.1"
      effective_io_concurrency: "200"
      work_mem: "72315kB"
      huge_pages: "try"
      min_wal_size: "4GB"
      max_wal_size: "16GB"
      max_worker_processes: "16"
      max_parallel_workers_per_gather: "8"
      max_parallel_workers: "16"
      max_parallel_maintenance_workers: "4"
    pg_hba:
      # - host all all 10.244.0.0/16 md5
      - host all all all md5

  bootstrap:
    recovery:
      source: s3
      database: postgres
      owner: app
      secret:
        name: gearhawk-postgres-app

  enableSuperuserAccess: true
  superuserSecret:
    name: gearhawk-postgres-superuser

  storage:
    # TODO: benchmark and use local storage
    storageClass: longhorn
    size: 100Gi

  managed:
    roles:
      - name: gearhawk-pipeline-worker
        ensure: present
        login: true
        superuser: false
        createdb: true
        createrole: false
        inherit: false
        replication: false
        bypassrls: false
        passwordSecret:
          name: gearhawk-pipeline-worker
  externalClusters:
    - name: s3
      plugin:
        name: barman-cloud.cloudnative-pg.io
        parameters:
          # Recovery Object Store (pull, read-only)
          barmanObjectName: s3-store
          serverName: recovery-target # the target path where base + wal objects are stored, take second last base + all wal
  plugins:
    - name: barman-cloud.cloudnative-pg.io
      isWALArchiver: true
      parameters:
        # Backup Object Store (push, read-write)
        barmanObjectName: s3-store
        serverName: scheduled-backup # this s3 path must be empty during recovery

  resources:
    requests:
      memory: "32Gi"
      cpu: "4"
    limits:
      memory: "64Gi"
      cpu: "16"
  monitoring:
    enablePodMonitor: true
  # TODO:
  # affinity:
  #   enablePodAntiAffinity: true
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: kubernetes.io/hostname
                operator: In
                values:
                  - work-00

  nodeMaintenanceWindow:
    inProgress: false
    reusePVC: false
